\documentclass[journal,11pt]{IEEEtran}
\usepackage[margin=0.5in]{geometry}
% *** CITATION PACKAGES ***
\usepackage[style=ieee]{biblatex} 
\bibliography{example_bib.bib}    %your file created using JabRef

% *** MATH PACKAGES ***
\usepackage{amsmath}

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}  %needed to include png, eps figures
\usepackage{float}  % used to fix location of images i.e.\begin{figure}[H]

\begin{document}
% paper title
\title{Regression Models for Predicting Weekly New COVID-19 Hospitalized Cases}

% author names 
\author{Zebang Yu, 260825575,
        Junjie Yang, 260829732,
        Maoyu Wang, 260846952
        }% <-this % stops a space
% The report headers
\markboth{K-Nearest Neighbour and Decision Tree Model}%do not delete next lines
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This work studies different regression models' performance on predicting weekly new COVID-19 Hospitalized cases, based on the google search patterns of health symptoms. We begin with preprocessing the COVID-19 Search Trends symptoms dataset and COVID hospitalization cases dataset, made publicly available by Google research. Next, we investigate search patterns by finding the most popular search and visualizing the data with PCA and K-means clustering method. We employ K-Nearest Neighbour and Decision Tree regressors to predict the hospitalization cases given the search trends data. We discuss the performance difference between the two models. After training both models, We found that the k-nearest neighbour regression approach achieved better accuracy than decision trees. When we see the root mean square error (RMSE) scores, it is interesting that the newly hospitalized patients are proportional to the time. As we try the time-split strategy, the result became highly unstable.

\end{abstract}


\section{Introduction}
% Here we have the typical use of a "W" for an initial drop letter
% and "RITE" in caps to complete the first word.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

\IEEEPARstart{K}{Nearest} Neighbour (KNN) and Decision Tree are two simple models for classification and regression problems. KNN is known for reliability in tackling issues where the true relationship between variables is unknown. We employed both models to predict the weekly new COVID-19 Hospitalized cases and compared their performance on multiple metrics in this work. \\
\indent We use the K-fold cross-validation technique during the training process. During the training process, we split the dataset by regions and times. Using K-fold allows us to modify our KNN and Decision Tree models to be fairer and more precise.\\
\indent After we trained both models, it is noticeable that K Nearest Neighbor Regression achieves better results for both region-split strategy and time-split strategy. However, the decision tree becomes unstable when handling with the time data. It is also interesting that if we divide our dataset by time, both models became unstable for the validation set. This may suggest that the number of newly hospitalized patients may be proportional to the time. As a result, we achieved 28.15 for region-split KNN with k = 7, 36.20 for time-split KNN with k = 5, 26 for region-split Decision Tree with maximum depth =2, and 35 for time-split Decision Tree with maximum depth = 13.


\section{Datasets}
For this mini-project, one of the datasets used focuses on the trends in search patterns for symptoms. The dataset is generated by Google to help researchers to understand the impact of COVID-19 better. The dataset consists of 430 columns, including the geometric information, date, and the rest of the columns are representing symptom names. However, after reviewing several rows of the dataset, it is clear that some columns can be dropped for reducing the dimensionality of the data frame. In the dataset, the first six columns represent the area where the symptoms were searched. As we only need 1 column as the index. We keep sub\_region\_1 as the index and drop the rest 5 to reduce the dimensionality. Then, we examine the symptoms.  As many symptoms have no search trend, it is safe to remove them from the dataset to increase the model's correctness. Thus, we bring the size of our dataset from 624 x 430 to 624 x 123.
\newline \indent The second dataset used is the COVID hospitalization cases dataset. NEED TO BE IMPLEMENTED!!!

\section{Visualization}
To better understand the search trend dataset, we employ multiple strategies to visualize the data. First, we visualized the evolution of the popularity of various symptoms across different regions over time. We picked the three most popular symptoms searched, namely, Aphonia, viral pneumonia, shallow breathing. Next, we visualized the search trends dataset in a lower-dimensional space. Specifically, we employed the Principle Component Analysis (PCA) to handle the dimensionality reduction. We found that $95\%$ variance among the data can be explained with the top $20$ principal components. 
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{PCA.png}
\label{fig:ecg}
\end {center}
\end{figure}
We plot the data points based on the top $2$ principal components. This depicts how our dimensionality-reduced data look like in 2D. 
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{PCA2.png}
\label{fig:ecg}
\end {center}
\end{figure}
Finally, we used k-means to evaluate possible groups in the search trends dataset. We found that the clusters remain consistent for raw as well as PCA-reduced data, with $k=5$. 
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{PCA_cluster.png}
\label{fig:ecg}
\end {center}
\end{figure}

\section{Supervised Learning}
To improve the models' consistency and accuracy, we partitioned the dataset based on regions and dates. That is, we split the data into train and validation sets using these two strategies. We perform cross-validation for the region-based grouping with around $80\%$ regions in the training set and the remaining $20\%$ in the validation set. As for the time-based strategy, the training set is all the data points with the date before `2020-08-10', and the remaining ones are in the validation set.
\begin{center}
    \subsection{KNN Model}
\end{center}
\indent \indent We first performed $5$-fold cross-validation for each choice of $k$ between $1$ to $20$ with region-split strategy. We found that $k=7$ is optimal according to the elbow rule. The RMSE of the model on the training set and validation set are $28.15$ and $14.3$ respectively. 

\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{knn_region.png}
\label{fig:ecg}
\end {center}
\end{figure}
For the partition based on time, we observe that $k=5$ gives the optimal prediction in this case, where the RMSE on the training set and validation set are $36.20$ and $10.36$, respectively. We found that the RMSE on the validation set is significantly higher than the region-split strategy for all choices of $k$. This implies time itself could be a variable that affects the evolution of new hospitalized cases.
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{knn_time.png}
\label{fig:ecg}
\end {center}
\end{figure}
\begin{center}
    \subsection{Decision Tree Model}
\end{center}
\indent \indent The critical parameter of decision tree models is the maximum depth of the leaf nodes. Hence we vary the maximum depth to select the one that gives the lowest RMSE. We performed cross-validations for each choice and found that the tree with height at most $2$ had the best performance.
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{dt_region.png}
\label{fig:ecg}
\end {center}
\end{figure}
Again, if we do the training-validation split by time, the RMSE soared like the KNN model.
\begin{figure}[H]%[!ht]
\begin {center}
\includegraphics[width=0.45\textwidth]{dt_time.png}
\label{fig:ecg}
\end {center}
\end{figure}
\section{Discussion and Conclusion}
In this work, we focused on using two different regression models, K Nearest Neighbor Regressor and Decision Tree Regressor, on predicting the number of newly hospitalized patients. We also analyzed these models further, specifically the K Nearest Neighbor model, determining the optimal k, which distance function we should use, and what weight we should use for better results. Yet, by using Euclidean distance and uniform weight, we get the best result among all the possible combinations of the parameters. To get a better result, we tried another model, namely the Random Forest Regressor. 
 \\NOT FINISHED HERE!!!\\
\indent However, this work can be improved in the future. For example, in this project, we only use the hospitalized\_new field from the second dataset. Other fields in the second dataset may also be helpful to show the impact of COVID-19. In addition to that, we have not tuned the parameters of our implemented model. If we apply GridSearchCV on all of our models, we may get a better result in return.\\
Furthermore, after cleaning the data, it is noticeable that many rows consist of consecutive NaNs. If we can get a more comprehensive dataset, we may increase our models' accuracy a lot. Last, we may explore a more suitable algorithm for the dataset in the future, which will also increase the accuracy of the prediction.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%
\section{Statement of Contributions}
\textbf{Zebang Yu}: Dataset cleaning, visualizing, dimension-reduction, model selection and performance evaluation, report writing
\newline\indent \textbf{Junjie Yang}: Acquire both datasets, clean the datasets, produce several graphs, implement KNN regression model. Wrote part of the report.
\newline\indent \textbf{Maoyu Wang}:
\appendices
\section{Hand calculations (or name your title for appendix subtitle)}

% use section* for acknowledgment
\section*{Acknowledgment}
The authors would like to thank...



% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%use following command to generate the list of cited references

\printbibliography


Example of Internet web page:\\[0.1in]
[1] Google LLC "Google COVID-19 Search Trends symptoms dataset".
http://goo.gle/covid19symptomdataset, Accessed: Oct. 9,2020.

% List and number all bibliographical 
% references at the end of your paper in {\bf 9 or 10 point} Times, with 10-point interline spacing. When referenced within the text, enclose the citation number in square brackets, for example [1]. \\
% Use IEEE format. Cite any external work that you used (data sheets, text books, Wikipedia articles, . . . ). If you get a formula from a Wikipedia article, you must cite the article, giving the title, the URL, and the data you accessed the article as a minimum. If you copy a figure, not only must you cite the article you copied from, but you must give explicit figure credit in the caption for the figure: This image copied from . . . . If you modify a figure or base your figure on one that has been published elsewhere, you still need to give credit in the caption: This image adapted from . . . .\\[0.1in]

% that's all folks
\end{document}



